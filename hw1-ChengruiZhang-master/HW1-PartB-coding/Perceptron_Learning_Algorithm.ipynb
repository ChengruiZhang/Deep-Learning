{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning Algorithm\n",
    "\n",
    "The perceptron is a simple supervised machine learning algorithm and one of the earliest neural network architectures. It was introduced by Rosenblatt in the late 1950s. A perceptron represents a binary linear classifier that maps a set of training examples (of $d$ dimensional input vectors) onto binary output values using a $d-1$ dimensional hyperplane. But Today, we will implement **Multi-Classes Perceptron Learning Algorithm** \n",
    "**Given:**\n",
    "* dataset $\\{(x^i, y^i)\\}$, $i \\in (1, M)$\n",
    "* $x^i$ is $d$ dimension vector, $x^i = (x^i_1, \\dots x^i_d)$\n",
    "* $y^i$ is multi-class target varible $y^i \\in \\{0,1,2\\}$\n",
    "\n",
    "A perceptron is trained using gradient descent. The training algorithm has different steps. In the beginning (step 0) the model parameters are initialized. The other steps (see below) are repeated for a specified number of training iterations or until the parameters have converged.\n",
    "\n",
    "**Step0:** Initial the weight vector and bias with zeros     \n",
    "**Step1:** Compute the linear combination of the input features and weight. $y^i_{pred} = argmax_k W_k*x^i + b$    \n",
    "**Step2:** Compute the gradients for parameters $W_k$, $b$. **Derive the parameter update equation Here(5 points)**   \n",
    "\n",
    "##################################     \n",
    "TODO: Derive you answer hear\n",
    "#################################\n",
    "                              \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "print(type(X))\n",
    "y = iris.target\n",
    "y = np.array(y)\n",
    "print('X_Shape:', X.shape)\n",
    "print('y_Shape:', y.shape)\n",
    "print('Label Space:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the training set and test set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)\n",
    "print('X_train_Shape:', X_train.shape)\n",
    "print('X_test_Shape:',  X_test.shape)\n",
    "print('y_train_Shape:', y_train.shape)\n",
    "print('y_test_Shape:',  y_train.shape)\n",
    "\n",
    "print(type(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClsPLA(object):\n",
    "    \n",
    "    ## We recommend to absorb the bias into weight.  W = [w, b]\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_test, y_test, lr, num_epoch, weight_dimension, num_cls):\n",
    "        super(MultiClsPLA, self).__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.weight = self.initial_weight(weight_dimension, num_cls)\n",
    "        self.sample_mean = np.mean(self.X_train, 0)\n",
    "        self.sample_std = np.std(self.X_train, 0)\n",
    "        self.num_epoch = num_epoch\n",
    "        self.lr = lr\n",
    "        self.total_acc_train = []\n",
    "        self.total_acc_tst = []\n",
    "          \n",
    "    def initial_weight(self, weight_dimension, num_cls):\n",
    "        weight = None\n",
    "        #########################################\n",
    "        ##  ToDO: Initialize the weight with   ##\n",
    "        ##  samll std and zero mean gaussian   ##\n",
    "        #########################################\n",
    "     \n",
    "        return weight\n",
    "        \n",
    "    def data_preprocessing(self, data):\n",
    "        #####################################\n",
    "        ##  ToDO: Normlize the data        ##\n",
    "        #####################################\n",
    "        norm_data = (data-self.sample_mean)/self.sample_std\n",
    "        return norm_data\n",
    "    \n",
    "    def train_step(self, X_train, y_train, shuffle_idx):\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        X_train = X_train[shuffle_idx]\n",
    "        y_train = y_train[shuffle_idx]\n",
    "        train_acc = None\n",
    "        ##############################################\n",
    "        ## TODO: to implement the training process  ##\n",
    "        ## and update the weights                   ##\n",
    "        ##############################################\n",
    "        \n",
    "        \n",
    "        return train_acc\n",
    "        \n",
    "    def test_step(self, X_test, y_test):\n",
    "        \n",
    "        \n",
    "        X_test = self.data_preprocessing(data=X_test)\n",
    "        num_sample = X_test.shape[0]\n",
    "        test_acc = None\n",
    "        \n",
    "        #########################################\n",
    "        ##  ToDO: Evaluate the test set and    ##\n",
    "        ##  return the test acc                ##\n",
    "        #########################################\n",
    "        \n",
    "           \n",
    "        return test_acc\n",
    "        \n",
    "    def train(self):\n",
    "           \n",
    "        self.X_train = self.data_preprocessing(data=self.X_train)\n",
    "        num_sample = self.X_train.shape[0]\n",
    "        \n",
    "        ######################################################\n",
    "        ### TODO: In order to absorb the bias into weights ###\n",
    "        ###  we need to modify the input data.             ###\n",
    "        ###  So You need to transform the input data       ###\n",
    "        ######################################################\n",
    "         \n",
    "        \n",
    "        shuffle_index = np.array(range(0, num_sample))\n",
    "        for epoch in range(self.num_epoch):\n",
    "            training_acc = self.train_step(X_train=self.X_train, y_train=self.y_train, shuffle_idx=shuffle_index)\n",
    "            tst_acc = self.test_step(X_test=self.X_test,  y_test=self.y_test)\n",
    "            self.total_acc_train.append(training_acc)\n",
    "            self.total_acc_tst.append(tst_acc)\n",
    "            print('epoch:', epoch, 'traing_acc:%.3f'%training_acc, 'tst_acc:%.3f'%tst_acc)\n",
    "    \n",
    "    def vis_acc_curve(self):\n",
    "        train_acc = np.array(self.total_acc_train)\n",
    "        tst_acc = np.array(self.total_acc_tst)\n",
    "        plt.plot(train_acc)\n",
    "        plt.plot(tst_acc)\n",
    "        plt.legend(['train_acc', 'tst_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "#######################################################\n",
    "### TODO: \n",
    "### 1. You need to import the model and pass some parameters. \n",
    "### 2. Then training the model with some epoches.\n",
    "### 3. Visualize the training acc and test acc verus epoches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
